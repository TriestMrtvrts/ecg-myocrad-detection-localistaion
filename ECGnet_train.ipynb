{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Загрузка датасета\n",
        "%%capture\n",
        "!gdown 1NhdUKxIbUUtmaY5avSUoNjPnEjlZfjF5\n",
        "!unzip \"simple_r_peaks.zip\""
      ],
      "metadata": {
        "id": "JrFF4ZSLkinK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install timm torchmetrics wandb focal_loss_torch"
      ],
      "metadata": {
        "id": "3DGZj71JhUWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mlQ2cL0LwuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70732787-c78a-4b43-c91b-5e995523066c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as ptl\n",
        "import torchmetrics\n",
        "import matplotlib.pyplot as plt\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from IPython.display import clear_output\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision import models, transforms\n",
        "from focal_loss.focal_loss import FocalLoss\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KCMRfSuyRPs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login\n",
        "import wandb\n",
        "\n",
        "\n",
        "wandb.init(\n",
        "    project=\"aiijc_final\",\n",
        "\n",
        "    config={\n",
        "        \"architecture\": \"ecg_net\",\n",
        "        \"dataset\": \"simple_pseudo_ecgnet\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "A7_nb2l8hPi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/simple_r_peaks/labels.csv', index_col='Unnamed: 0')"
      ],
      "metadata": {
        "id": "UG4l68qFhgBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neZMFsCjXzbm"
      },
      "outputs": [],
      "source": [
        "class SignalsDataset(Dataset):\n",
        "    def __init__(self, labels, path='/'):\n",
        "        self.x_paths = [labels.iloc[i, 0] for i in range(len(labels))]\n",
        "        self.labels = [labels.iloc[i, 1] for i in range(len(labels))]\n",
        "        self.path = path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        hr = torch.tensor(np.load(self.path + self.x_paths[idx] + '.npy'))[None, :, :]\n",
        "\n",
        "        target = self.labels[idx]\n",
        "\n",
        "        return hr, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94AbWvDoX-S-"
      },
      "outputs": [],
      "source": [
        "dataset = SignalsDataset(data, '/content/simple_r_peaks/signals/')\n",
        "train_data, val_data = random_split(dataset, [0.9, 0.1])\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DllrY_pb_-vq"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "class ECGNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ECGNet, self).__init__()\n",
        "    #layer1\n",
        "    self.layer1_conv2d = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(1, 25), stride=(1, 2), bias=True)\n",
        "\n",
        "\n",
        "    #layer2\n",
        "    self.layer2_conv2d = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm2d(num_features=32)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv2d(32, 64, kernel_size=(1, 15), stride=(1, 1), bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv2d(64, 64, kernel_size=(1, 15), stride=(1, 2),  bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv2d(64, 32, kernel_size=(1, 15), stride=(1, 1), bias=True)),\n",
        "    ]))\n",
        "    self.layer2_seModule = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv2d(32, 16, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv2d(16, 32, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    #layer3\n",
        "    self.layer3_conv2d_block1 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm2d(num_features=32)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv2d(32, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv2d(64, 32, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=True)),\n",
        "    ]))\n",
        "    self.layer3_seModule_block1 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv2d(32, 16, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv2d(16, 32, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    self.layer3_conv2d_block2 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm2d(num_features=32)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv2d(32, 64, kernel_size=(5, 1), padding=(2, 0), bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv2d(64, 64, kernel_size=(5, 1), padding=(2, 0), bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv2d(64, 32, kernel_size=(5, 1), padding=(2, 0), bias=True)),\n",
        "    ]))\n",
        "    self.layer3_seModule_block2 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv2d(32, 16, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv2d(16, 32, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    self.layer3_conv2d_block3 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm2d(num_features=32)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv2d(32, 64, kernel_size=(7, 1), padding=(3, 0), bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv2d(64, 64, kernel_size=(7, 1), padding=(3, 0), bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv2d(64, 32, kernel_size=(7, 1), padding=(3, 0), bias=True)),\n",
        "    ]))\n",
        "    self.layer3_seModule_block3 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv2d(32, 16, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv2d(16, 32, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    #layer4\n",
        "    self.layer4_conv1d_short_block1 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 384, kernel_size=3, stride=9, bias=True)),\n",
        "    ]))\n",
        "\n",
        "    self.layer4_conv1d_block1 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 768, kernel_size=3, stride=2, bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv1d(768, 768, kernel_size=3, stride=1, bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv1d(768, 1536, kernel_size=3, stride=2, bias=True)),\n",
        "        (\"bn4\", nn.BatchNorm1d(num_features=1536)),\n",
        "        (\"act4\", nn.ReLU()),\n",
        "        (\"cn4\", nn.Conv1d(1536, 384, kernel_size=3, stride=2, bias=True)),\n",
        "    ]))\n",
        "    self.layer4_seModule_block1 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv1d(384, 48, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv1d(48, 384, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    self.layer4_conv1d_short_block2 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 384, kernel_size=5, stride=9, bias=True)),\n",
        "    ]))\n",
        "\n",
        "    self.layer4_conv1d_block2 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 768, kernel_size=5, stride=2, padding=2, bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv1d(768, 768, kernel_size=5, stride=2, padding=1, bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv1d(768, 1536, kernel_size=5, stride=1, padding=2, bias=True)),\n",
        "        (\"bn4\", nn.BatchNorm1d(num_features=1536)),\n",
        "        (\"act4\", nn.ReLU()),\n",
        "        (\"cn4\", nn.Conv1d(1536, 384, kernel_size=5, stride=2, padding=1, bias=True)),\n",
        "    ]))\n",
        "    self.layer4_seModule_block2 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv1d(384, 48, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv1d(48, 384, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    self.layer4_conv1d_short_block3 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 384, kernel_size=7, stride=9, bias=True)),\n",
        "    ]))\n",
        "\n",
        "    self.layer4_conv1d_block3 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 768, kernel_size=7, stride=2, padding=2, bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv1d(768, 768, kernel_size=7, stride=2, padding=1, bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv1d(768, 1536, kernel_size=7, stride=1, padding=3, bias=True)),\n",
        "        (\"bn4\", nn.BatchNorm1d(num_features=1536)),\n",
        "        (\"act4\", nn.ReLU()),\n",
        "        (\"cn4\", nn.Conv1d(1536, 384, kernel_size=7, stride=2, padding=2, bias=True)),\n",
        "    ]))\n",
        "    self.layer4_seModule_block3 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv1d(384, 48, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv1d(48, 384, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    self.layer5_avg_pool1 = nn.AvgPool1d(kernel_size=10)\n",
        "    self.layer5_avg_pool2 = nn.AvgPool1d(kernel_size=10)\n",
        "    self.layer5_avg_pool3 = nn.AvgPool1d(kernel_size=10)\n",
        "\n",
        "    self.fc = nn.Sequential(OrderedDict([\n",
        "        (\"ln1\", nn.Linear(1152, 288)),\n",
        "        (\"dp\", nn.Dropout(p=0.2)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"ln2\", nn.Linear(288, 7))\n",
        "    ]))\n",
        "\n",
        "  def forward(self, x):\n",
        "    #layer1\n",
        "    x = self.layer1_conv2d(x)\n",
        "\n",
        "    #layer2\n",
        "    x = self.layer2_conv2d(x)\n",
        "    u = x\n",
        "    x = x.view(x.size(0), x.size(1), -1).mean(-1).view(x.size(0), x.size(1), 1, 1)\n",
        "    x = self.layer2_seModule(x)\n",
        "    x = u * x\n",
        "\n",
        "    #layer3\n",
        "    x1 = self.layer3_conv2d_block1(x)\n",
        "    u1 = x1\n",
        "    x1 = x1.view(x1.size(0), x1.size(1), -1).mean(-1).view(x1.size(0), x1.size(1), 1, 1)\n",
        "    x1 = self.layer3_seModule_block1(x1)\n",
        "    x1 = u1 * x1\n",
        "\n",
        "    x2 = self.layer3_conv2d_block2(x)\n",
        "    u2 = x2\n",
        "    x2 = x2.view(x2.size(0), x2.size(1), -1).mean(-1).view(x2.size(0), x2.size(1), 1, 1)\n",
        "    x2 = self.layer3_seModule_block2(x2)\n",
        "    x2 = u2 * x2\n",
        "\n",
        "    x3 = self.layer3_conv2d_block3(x)\n",
        "    u3 = x3\n",
        "    x3 = x3.view(x3.size(0), x3.size(1), -1).mean(-1).view(x3.size(0), x3.size(1), 1, 1)\n",
        "    x3 = self.layer3_seModule_block3(x3)\n",
        "    x3 = u3 * x3\n",
        "\n",
        "    #layer4\n",
        "    x1 = torch.flatten(x1, start_dim=1, end_dim=2)\n",
        "    x2 = torch.flatten(x2, start_dim=1, end_dim=2)\n",
        "    x3 = torch.flatten(x3, start_dim=1, end_dim=2)\n",
        "\n",
        "    # x1 = x1.unsqueeze(1)\n",
        "    # x2 = x2.unsqueeze(1)\n",
        "    # x3 = x3.unsqueeze(1)\n",
        "\n",
        "    x1_short = self.layer4_conv1d_short_block1(x1)\n",
        "\n",
        "    x1 = self.layer4_conv1d_block1(x1)\n",
        "    u1 = x1\n",
        "    x1 = x1.view(x1.size(0), x1.size(1), -1).mean(-1).view(x1.size(0), x1.size(1), 1, 1).flatten(2, 3)\n",
        "    x1 = self.layer4_seModule_block1(x1)\n",
        "    x1 = u1 * x1\n",
        "    x1 = x1 + x1_short\n",
        "\n",
        "    x2_short = self.layer4_conv1d_short_block2(x2)\n",
        "\n",
        "    x2 = self.layer4_conv1d_block2(x2)\n",
        "    u2 = x2\n",
        "    x2 = x2.view(x2.size(0), x2.size(1), -1).mean(-1).view(x2.size(0), x2.size(1), 1, 1).flatten(2, 3)\n",
        "    x2 = self.layer4_seModule_block2(x2)\n",
        "    x2 = u2 * x2\n",
        "    x2 = x2 + x2_short\n",
        "\n",
        "    x3_short = self.layer4_conv1d_short_block3(x3)\n",
        "\n",
        "    x3 = self.layer4_conv1d_block3(x3)\n",
        "    u3 = x3\n",
        "    x3 = x3.view(x3.size(0), x3.size(1), -1).mean(-1).view(x3.size(0), x3.size(1), 1, 1).flatten(2, 3)\n",
        "    x3 = self.layer4_seModule_block3(x3)\n",
        "    x3 = u3 * x3\n",
        "    x3 = x3 + x3_short\n",
        "\n",
        "    x1 = self.layer5_avg_pool1(x1)\n",
        "    x2 = self.layer5_avg_pool2(x2)\n",
        "    x3 = self.layer5_avg_pool3(x3)\n",
        "\n",
        "    x = torch.cat((x1, x2, x3), dim=1).flatten(1)\n",
        "\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9OlBkR_JCIP"
      },
      "outputs": [],
      "source": [
        "model = ECGNet()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wwv6nm8BYvY0"
      },
      "outputs": [],
      "source": [
        "loss_fn = FocalLoss(gamma=2)\n",
        "\n",
        "learning_rate = 3e-5\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWFE7-GeZXJ8"
      },
      "outputs": [],
      "source": [
        "def train(model, loss_fn, scheduler, optimizer, n_epoch=3, device='cuda'):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "    val_f1 = []\n",
        "\n",
        "    max_f1 = 0\n",
        "\n",
        "    val_accuracy, val_loss, val_f1_score = evaluate(model, val_loader, loss_fn=loss_fn, device=device)\n",
        "    wandb.log({\"F1\": val_f1_score, \"Acc\": val_accuracy, 'loss': val_loss})\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "        print(\"Epoch:\", epoch+1)\n",
        "\n",
        "        model = model.train()\n",
        "        for batch in tqdm(train_loader):\n",
        "            X_batch, y_batch = batch\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch.float())\n",
        "\n",
        "            logits = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "            loss = loss_fn(logits, y_batch.to(torch.int64))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            model_answers = torch.argmax(logits, dim=1).to(torch.int64)\n",
        "\n",
        "            train_accuracy = torch.sum(y_batch == model_answers) / len(y_batch)\n",
        "            train_acc.append(train_accuracy.item())\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        val_accuracy, val_loss, val_f1_score = evaluate(model, val_loader, loss_fn=loss_fn, device=device)\n",
        "        wandb.log({\"F1\": val_f1_score, \"Acc\": val_accuracy, 'loss': val_loss})\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        if  max_f1 < val_f1_score:\n",
        "            pass\n",
        "            torch.save(model.state_dict(), f'/content/drive/MyDrive/models/ecgNet_simpRp_Focal_loss_f1:{val_f1_score:.4f}_5000.pth')\n",
        "\n",
        "        val_losses.append(val_loss.item())\n",
        "        val_acc.append(val_accuracy)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "def evaluate(model, dataloader, loss_fn, device):\n",
        "    losses = []\n",
        "    num_correct = 0\n",
        "    num_elements = 0\n",
        "    f1 = torchmetrics.F1Score(task='multiclass', num_classes=7)\n",
        "    f1_score = 0\n",
        "\n",
        "    model = model.eval()\n",
        "    for batch in tqdm(dataloader):\n",
        "        X_batch, y_batch = batch\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device).float()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(X_batch.float())\n",
        "\n",
        "            logits = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "            loss = loss_fn(logits, y_batch.to(torch.int64))\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            y_pred = torch.argmax(logits, dim=1).to(torch.int64)\n",
        "\n",
        "            f1_score += f1(y_pred.cpu(), y_batch.cpu())\n",
        "\n",
        "            num_elements += len(y_batch)\n",
        "            num_correct += torch.sum(y_pred == y_batch)\n",
        "\n",
        "    accuracy = num_correct / num_elements\n",
        "    f1_score = f1_score / len(dataloader)\n",
        "\n",
        "    return accuracy.item(), np.mean(losses), f1_score.item()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(model=model, loss_fn=loss_fn, scheduler=scheduler, optimizer=optimizer, n_epoch=40)"
      ],
      "metadata": {
        "id": "ECuAB0ZmhDxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iI2rCig7NtIA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}